# Back translation for Task Adaptive Pre-training

ğŸ¤— Back translation: Papago ë²ˆì—­ê¸°ë¥¼ web crawling.  
ğŸ¤— Pre-trianer: HuggingFaceì˜ maksed language modelì„ Pre-train.    
ğŸ¤— Parameters: Best modelì— Pre-trained modelì„ load.
  
## Required Installations
```
pip install -r requirements.txt
```
- chromedriver

## How to use
### Back translation
```
# ê¸°ë³¸ ì‚¬ìš© ë°©ë²•
python back_translation.py

# nohupìœ¼ë¡œ log ë³´ê³  ì‹¶ìœ¼ë©´
bt.sh
```

**Options**
- --remove_stop_words: remove stop words (default: False)
- --only_kor_to_en: translate only kor to en (default: False)
- --only_en_to_kor: translate only en to kor (default: False)
- --len: specify length of csv file (default: False)

**Outputs**
- final_kor_to_eng_{file_time}.npy
- final_en_to_kor_{file_time}.npy
- back_translation_result.csv: contain kor_to_eng and eng_to_kor.

### Pre-training
```
python pretrain.py
```
Pre-trained model saved dir = './pretrined_model'.

### Load Pre-trained model for our best model
- Import back_trans/parameters.BackTransPreTrain in your model.
- MODEL_NAME: must use 'klue/roberta-large'
    - 'klue/roberta-large'ë¥¼ transformers.AutoModelë‚˜ transformers.AutoModelForMaskedLMë¡œ loadí•´ì•¼ì§€ë§Œ pre-trained modelì„ ì‚¬ìš© ê°€ëŠ¥.
```python
from back_trans import BackTransPreTrain

model = AutoModel.from_pretrained(MODEL_NAME)

bpt = BackTransPreTrain(pretrain_path)
model.load_state_dict(bpt.load_parameters(MODEL_NAME))
```
